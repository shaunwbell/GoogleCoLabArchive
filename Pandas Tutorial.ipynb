{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed for this analysis\n",
    "from copy import copy, deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a new library **re** (short for regular expressions or regex). This is one of the go-to libraries for text parsing.\n",
    "\n",
    "https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "Even though you are likely to primarily use numeric data, I suggest reading through this if possible. You'll find, in your future coding, text parsing comes up quite often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy indexing\n",
    "The indexing in a numpy array is similar to indexing a set/list. However, arrays can be multi-dimensional, so each dimension can be handled separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "print('The matrix a:\\n', a, '\\n')\n",
    "print('The shape of a is: ', a.shape, '\\n')  # print the shape of a\n",
    "print('The first row of a is: ', a[0], '\\n')  # print the first row of a (all columns)\n",
    "print('Slicing all but the first row of a:\\n', a[1:], '\\n') # print all but the first row of a (all columns)\n",
    "print('The second column of a is: ', a[:, 1], '\\n')  # print the second column of a (all rows)\n",
    "print('The second and third values of the third column are: ', a[1:3, 2], '\\n')  # print all but the first row of a but only the third column\n",
    "print('The transpose of a is:\\n', a.T)  # transpose a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use boolean indexing with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_index_1 = [True if any([True if i % 5 == 0 else False for i in row]) else False for row in a]\n",
    "print('Boolean of which rows in a have at least one value that is exactly divisible by 5: ', bool_index_1)\n",
    "\n",
    "print('The rows in a that have at least one value that is exactly divisible by 5:\\n', a[bool_index_1], '\\n')\n",
    "\n",
    "bool_index_2 = [[True if i % 2 == 0 else False for i in row] for row in a]\n",
    "print('Boolean of which values in a are even: ', bool_index_2, '\\n')\n",
    "\n",
    "print('An array of the values in a that are even:\\n', a[np.array(bool_index_2)],'\\n')  # note that bool_index_2 needs to ba an array for numpy to understand the format\n",
    "\n",
    "print(\"We can do this using an even simpler method directly using a[a % 2 == 0]:\\n\", a[a % 2 == 0], '\\n')\n",
    "\n",
    "print(\"We can also easily replace values:\")\n",
    "b = copy(a)  # make a copy of a\n",
    "b[~(a % 2 == 0)] = 0  # set all non-even values in b to 0\n",
    "\n",
    "print('Setting all odd values in a to zero:\\n', b)\n",
    "\n",
    "b[~np.array(bool_index_2)] = float('nan')  # note that this does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that numpy is particular about typing. All values in an array must be of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.astype(float)  # you can use .astype(...) to change the type of the values in an array\n",
    "print(b, '\\n')\n",
    "b[~np.array(bool_index_2)] = float('nan')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few useful numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all of the unique values in a list/array. This can be quite useful for filtering and building filters.\n",
    "print(np.unique([1, 7, 2, 1, 5, 3, 4, 4, 7, 9, 1, 5, 7, 6, 8, 9]), '\\n')\n",
    "\n",
    "# create an array of incrementing integers\n",
    "c = np.arange(1, 10)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas indexing\n",
    "Pandas is built on Numpy but indexing in Pandas is a little different. As columns are named, they are easier to access. Row/column combinations require a bit different syntax compared to numpy. Pandas also doesn't require all values in a dataframe to be of the dame type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We can turn an array into a dataframe and specify column names:\")\n",
    "df = pd.DataFrame(a, columns=['a', 'b', 'c'])\n",
    "print(df, '\\n')\n",
    "\n",
    "print(\"We can also use a dictionary to produce a dataframe. In this case, the keys become the column names.\")\n",
    "a_again = {'a': [1, 4, 7, 10], 'b': [2, 5, 8, 11], 'c': [3, 6, 9, 12]}\n",
    "print(a_again, '\\n')\n",
    "df_again = pd.DataFrame(a_again)\n",
    "print(df_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike with numpy, you can't do similar indexing in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[1, 'a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 different ways to index into a Pandas dataframe.\n",
    "\n",
    "The first is using the format: df[column name] or df[[column_names]] plus addtional indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting column 'a' using df['a']:\\n\", df['a'], '\\n')\n",
    "\n",
    "print(\"Getting columns 'a' and 'b' using df[['a', 'b']] (note the double brackets):\\n\", df[['a', 'b']], '\\n')\n",
    "\n",
    "print(\"Getting a slice from columns 'a' and 'b' using df[['a', 'b']][slice]:\\n\", df[['a', 'b']][1:3], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way is to use the df.column\\_name plus additional indexing. Note that this method can only be used for one column at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting column 'a' using df.a:\\n\", df.a, '\\n')\n",
    "\n",
    "print(\"Column 'b' rows 2-3 using .b[slice]:\\n\", df.b[1:3], '\\n')\n",
    "\n",
    "print(\"Column 'b' rows 2, 3, 4 using .b[[indicies]]:\\n\", df.b[[1, 2, 3]], '\\n')\n",
    "\n",
    "print(\"Getting rows based on a boolean index using .b[boolean_index]:\\n\", df.b[bool_index_1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other two methods are using .loc[...] and .iloc[...].\n",
    "\n",
    ".iloc[...] requires integer indicies for the rows and columns (so you can't use column names). However, it doesn't care how the rows and columns are named.\n",
    "\n",
    ".loc[...] can also use booleans or slices. However, the integers or slices must be based on the values of the indicies.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/loc-iloc-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column 'b' rows 2-3 using .loc[1:3, 'b']:\\n\", df.loc[1:3, 'b'], '\\n')\n",
    "\n",
    "print(\"Getting rows based on a boolean index using .loc[bool_index_1, :]:\\n\", df.loc[bool_index_1, :], '\\n')\n",
    "\n",
    "print(\"Getting rows based on row indicies using .loc[[1, 2, 3], :]:\\n\", df.loc[[1, 2, 3], :], '\\n')\n",
    "\n",
    "print(\"Getting rows based on the inequality df['b'] > 5 using .loc[df['b'] > 5, :] (note that df['b'] could be df.b):\\n\",\n",
    "      df.loc[df['b'] > 5, :], '\\n')\n",
    "\n",
    "print(\"Getting rows based on multiple inequalities using .loc[(df['b'] > 5) & (df['c'] < 10)]\\n\",\n",
    "      \"(note the format of (inequality 1) & (inequality 2) & ...):\\n\",\n",
    "      df.loc[(df['b'] > 5) & (df['c'] < 10)], '\\n')\n",
    "\n",
    "print('______________________________________________________________________________________________\\n')\n",
    "\n",
    "print(\"Rows 2-3 for all columns using integer indicies .iloc[[1, 2], :]\\n\", \n",
    "      \"(note, this does not work if you also select by column using column names):\\n\",\n",
    "      df.iloc[[1, 2], :])\n",
    "\n",
    "print(\"Rows 2-3 using a slice .iloc[1:3] (note, this also does not work if you also select by column using column name):\\n\",\n",
    "      df.iloc[1:3], '\\n')\n",
    "\n",
    "print(\"To also index by column using column names, you can use df.iloc[slice][column(s)]:\\n\", df.iloc[1:3]['b'], '\\n')\n",
    "\n",
    "print(\"Or you can use column integers using df.iloc[slice, column integer]:\\n\", df.iloc[1:3, [1, 2]], '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we change the row indicies, as in df_2, then .loc starts having trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_2 = deepcopy(df)\n",
    "\n",
    "df_2.index = ['m', 'n', 'o', 'p']\n",
    "\n",
    "print('New datafram df_2 with non-integer row indicies:\\n', df_2, '\\n')\n",
    "\n",
    "print(\"Rows 2-3 using .iloc[1:3] (note that the row indicies reset in the output):\\n\", df.iloc[1:3], '\\n')\n",
    "\n",
    "print(\"Column 'b' rows n and o using .loc[['n', 'o'], 'b']:\\n\", df_2.loc[['n', 'o'], 'b'], '\\n')\n",
    "\n",
    "# the examples below result in errors.\n",
    "print(\"Trying to index by row using standard integer indicies in .loc[...] (ex. df_2.loc[1:3, 'b']) fails.\")\n",
    "print(\"Column 'b' rows 2-3 using .loc[...]:\\n\", df_2.loc[1:3, 'b'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few useful functions (we will see more later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = pd.DataFrame(b, columns = ['a', 'b', 'c'])\n",
    "\n",
    "print(df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Boolean of NaN value positions:\\n\", df_b.isna(), '\\n')\n",
    "\n",
    "print(\"Let's see some information about df_b:\")\n",
    "df_b.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Count of number of NaNs in each column:\\n\", df_b.isna().sum(), '\\n')\n",
    "\n",
    "print(\"Fill the NaN values with something:\\n\", df_b.fillna(27), '\\n')\n",
    "\n",
    "print(\"Note, the above operation does not replace df_b, so if we call the dataframe again, we still have NaNs:\\n\", df_b, '\\n')\n",
    "\n",
    "print(\"To reset df_b, we must reallocate it to its namespace using df_b = df_b.fillna(27)\")\n",
    "df_b = df_b.fillna(27)\n",
    "print(df_b, '\\n')\n",
    "\n",
    "print(\"We can also change the type of column a using .astype()\")\n",
    "df_b['a'] = df_b['a'].astype(int)\n",
    "print(df_b.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .apply\n",
    "The .apply allows you to apply a function directly to a series or dataframe. The function must be able to handle all of the values together (so it must return either a single value or a series that is of the same shape as what was given to the function). The series/dataframe should be the first input to the function. Other inputs can be supplied with the args = (...). apply does not replace the values in the existing dataframe, rather creates a new dataframe or series. You will have to use df = df.apply(func) or df[column name] = df[column name].apply(func) to replace the existing values.\n",
    "\n",
    "https://www.geeksforgeeks.org/python-pandas-apply/\n",
    "\n",
    "### .map\n",
    "The .map function is similar to .apply, but applied to each value in a series or column (not the entire dataframe) independently. This is in contrast to .apply, which applies the function to the entire row/column/dataframe. To apply a function to each of the entries in an entire dataframe, you can use .applymap. Another major difference is .map and .applymap do not have the args parameter, so only simple functions that do not have more than one input can be used.\n",
    "\n",
    "https://towardsdatascience.com/introduction-to-pandas-apply-applymap-and-map-5d3e044e93ff#:~:text=And%20the%20Pandas%20official%20API%20reference%20suggests%20that:,each%20value%20in%20a%20Series%20with%20another%20value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_values(row):\n",
    "    return sum(row)\n",
    "\n",
    "def chlorophyll_volts_to_mgL(voltage, scale_factor, dark_count, global_correction=True):\n",
    "    \"\"\"\n",
    "    converts chlorophyll volts to ug/l\n",
    "    \n",
    "    parameters:\n",
    "    voltage: (float) a chlorophyll sensor voltage\n",
    "    scale_factor: (float) \n",
    "    \n",
    "    output:\n",
    "    converted_chl: (mg/L)\n",
    "    \"\"\"\n",
    "    if global_correction is True:\n",
    "        return round(scale_factor * (voltage - dark_count) / 2., 3)\n",
    "    else:\n",
    "        return round(scale_factor * (voltage - dark_count), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe before adding a new column that is the sum of the other columns:\\n\", df_b, '\\n')\n",
    "df_b['d'] = df_b.apply(sum_values, axis=1)  # axis=1 refers to summing along the columns\n",
    "print(\"Dataframe after adding the new column:\\n\", df_b, '\\n')\n",
    "\n",
    "print(\"We can add another row with the sums if we specify axis=0 (note that there is some additional code):\\n\")\n",
    "df_b = df_b.append(df_b.apply(sum_values), ignore_index=True)\n",
    "\n",
    "print(df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column a, before conversion:\\n\", df_b['a'], '\\n')\n",
    "df_b['a'] = df_b['a'].apply(chlorophyll_volts_to_mgL, args=(5, 1))\n",
    "print(\"Column a, after conversion:\\n\", df_b['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We can try using .apply(func) on the entire dataframe.\")\n",
    "print(df_b.apply(chlorophyll_volts_to_mgL, args=(5, 1)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"We can try using .applymap(func) on the entire dataframe.\n",
    "We can use a lambda function to mimic the chlorophyll_volts_to_mgl function.\"\"\")\n",
    "print(df_b.applymap(lambda x: 5 * (x - 1)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving\n",
    "**to_csv** will likely be your go to way to save a dataframe.\n",
    "\n",
    "The standard output is comma delimited (csv). index=False does not save the index column of your dataframe. If you have replaced the standard index with a new index, omit the index=False.\n",
    "> df.to_csv('filename.csv', index=False) if you want to save to the same folder as your program.\n",
    "\n",
    "> df.to_csv('folderpath/filename.csv', index=False) if you want to save to a different folder than your program.\n",
    "\n",
    "If you want to write to a .txt file, use sep='\\t'. Again, omit index=False as necessary\n",
    "> df.to_csv('filename.txt', sep='\\t', index=False) if you want to save to the same file as your program.\n",
    "\n",
    "> df.to_csv('folderpath/filename.txt', sep='\\t', index=False) if you want to save to a different folder than your program.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b.to_csv('Dataframe b.csv', index=False)\n",
    "df_b.to_csv('Dataframe b.txt', sep='\\t')  # I omit index=False here so that you can see the difference in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "**read_csv** will likely be your standard way to load data. Not that the first row of your file will be your column names. If you do not want that to happen, use header=None.\n",
    "\n",
    "If you want to read in a csv:\n",
    "> pd.read_csv('filename.csv') if your file is in the same folder as your program.\n",
    "\n",
    ">pd.read_csv('folderpath\\filename.csv') if your file is in a different folder than your program.\n",
    "\n",
    "If you want to read in txt files, you can specify **sep='\\s+'** or **sep='\\t'**. If you use the **\\s+**, any spaces will be used to make new columns (conjoined spaces are treated as one). For example, '10/24/1918 12:45' will result in two columns, not one. If you use the **\\t**, only tabs will be used to make new columns. For the **\\t**, you will need to add **engine='python'** otherwise you will get a warning.\n",
    "> pd.read_csv('filename.txt', sep='\\s') if your file is in the same folder as your program.\n",
    "\n",
    "> pd.read_csv('folderpath\\filename.csv', sep='\\t', engine='python') if your file is in a different folder than your program.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading a standard csv:\")\n",
    "df_b_load1 = pd.read_csv('Dataframe b.csv')\n",
    "print(df_b_load1, '\\n')\n",
    "\n",
    "print(\"Loading a text file, specifying no header, and specifying an index column:\")\n",
    "df_b_load2 = pd.read_csv('Dataframe b.txt', sep='\\s', header=None, index_col=0, engine='python')\n",
    "print(df_b_load2, '\\n')\n",
    "print(df_b_load2.info(), '\\n')\n",
    "\n",
    "print(\"Reloading the text file, with a header, and specifying an index column:\")\n",
    "df_b_load3 = pd.read_csv('Dataframe b.txt', sep='\\s+')  # note that we don't need to specify index_col=0 either.\n",
    "print(df_b_load3, '\\n')\n",
    "print(df_b_load3.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEO data example\n",
    "- Downloading data via url\n",
    "- Creating a dataframe using pandas\n",
    "- Calculating monthly means and standard deviations\n",
    "- Plotting using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for data download via url\n",
    "\n",
    "def download_url(url, save_file_name):\n",
    "    \"\"\"Downloads data from a url and saves it in a local text file.\n",
    "    Parameters\n",
    "    ------------\n",
    "    url: (str) url of the text to be downloaded\n",
    "    save_file_name: (str) name of the save file. '.txt' will be appended if not already specified.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"downloading: \",url)\n",
    "    save_file_name = save_file_name if save_file_name.endswith('.txt') else save_file_name + '.txt'\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        with open(save_file_name, 'wb') as f:\n",
    "          for data in r:\n",
    "            f.write(data)\n",
    "        \n",
    "def load_textfile_to_pandas(file_name, comment_indicator=None): \n",
    "    \"\"\"Loads a text file to a pandas dataframe. If there are comments at the begining of the file, and they are preceded\n",
    "    by a particular character (ex. #, \\, \") in each row, these rows can be omited by specifying the correct comment_indicator\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    file_name: (str) file name (if local) or file path if not local. '.txt' will be appended if not specified.\n",
    "    comment_indicator: (str) Character(s) preceding comments at the top of the text file. Each row of comments must start\n",
    "                       with the comment_indicator to be properly identified as a comment.\n",
    "    \n",
    "    Output\n",
    "    ------------\n",
    "    Pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = file_name if file_name.endswith('.txt') else file_name + '.txt'\n",
    "    with open(file_name, 'r') as f:\n",
    "        count = 0\n",
    "        for data in f:\n",
    "            if comment_indicator and (len(data) == 0 or str(data).startswith(comment_indicator)):\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "    return pd.read_csv(file_name, sep='\\t', engine='python', skiprows=count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses above function to download data via url and save file\n",
    "\n",
    "station_name = 'KEO'\n",
    "file_name = 'KEO'\n",
    "comment_indicator = '#'\n",
    "\n",
    "download_url(\"https://www.pmel.noaa.gov/co2/timeseries/\" + station_name + \".txt\", file_name)\n",
    "keo = load_textfile_to_pandas(file_name, comment_indicator=comment_indicator)\n",
    "print(keo.head(), '\\n')\n",
    "print(keo.info(), '\\n')\n",
    "\n",
    "\n",
    "def convert_datetime(dtime_array):\n",
    "    \"\"\"Convert string to datetime object by searching possible datetime formats. Months must be mm and time hh:mm\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    dtime_array: (numpy array) array with datetime strings\n",
    "    \n",
    "    Output\n",
    "    ------------\n",
    "    list of datetime objects\n",
    "    \"\"\"\n",
    "    possible_time_formats = [\n",
    "                            '%m/%d/%Y', '%d/%m/%Y', '%Y/%m/%d', '%Y/%d/%m', '%m/%e/%Y', '%e/%m/%Y', '%Y/%m/%e', '%Y/%e/%m',\n",
    "                            '%m/%d/%y', '%d/%m/%y', '%y/%m/%d', '%y/%d/%m', '%m/%e/%y', '%e/%m/%y', '%y/%m/%e', '%y/%e/%m',\n",
    "                            '%m-%d-%Y', '%d-%m-%Y', '%Y-%m-%d', '%Y-%d-%m', '%m-%e-%Y', '%e-%m-%Y', '%Y-%m-%e', '%Y-%e-%m',\n",
    "                            '%m-%d-%y', '%d-%m-%y', '%y-%m-%d', '%y-%d-%m', '%m-%e-%y', '%e-%m-%y', '%y-%m-%e', '%y-%e-%m'\n",
    "                             ]\n",
    "    dtime = None\n",
    "    for time_format in possible_time_formats:\n",
    "        try:\n",
    "            dtime = [datetime.strptime(dt, time_format + ' %X') for dt in dtime_array]\n",
    "            break\n",
    "        except ValueError:\n",
    "            try:\n",
    "                dtime = [datetime.strptime(dt, time_format + ' %H:%M') for dt in dtime_array]\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "    if dtime:\n",
    "        return dtime\n",
    "    else:\n",
    "        raise ValueError('Could not convert datetime from given format')\n",
    "\n",
    "\n",
    "def column_to_datetime(df, column_name):\n",
    "    df[column_name] = convert_datetime(df[column_name])\n",
    "    return df\n",
    "\n",
    "keo = column_to_datetime(keo, 'datetime_utc')\n",
    "\n",
    "\n",
    "# changing the first column name\n",
    "keo.columns = ['dtime', 'SST', 'SSS', 'pCO2 sw', 'pCO2 air', 'xCO2 air', 'pH', 'DOXY', 'CHL', 'NTU']\n",
    "\n",
    "print(keo.head(), '\\n')\n",
    "print(keo.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting counts of unique values and counts of missing values\n",
    "df.nunique() will return the number of unique values per column. df.isna().sum() will return the number of NaN values per column.\n",
    "\n",
    "If you have categorical data, you might want to know how many instances of each category you have. You can check this with df['column name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check the number of unique values in each column with .nunique():\")\n",
    "print(keo.nunique(), '\\n')\n",
    "\n",
    "print(\"Check the number of NaNs in each column with .isna().sum():\")\n",
    "print(keo.isna().sum(), '\\n')\n",
    "\n",
    "print(\"Check the number of occurrences of each value of pCO2 air with .value_counts()\")\n",
    "print(keo['pCO2 air'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create datetime and month column from date and time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify number of rows to skip \n",
    "# this was done manually by opening file in Excel\n",
    "\n",
    "n = 110\n",
    "\n",
    "# read text file into dataframe\n",
    "keo_alt = pd.read_table('KEO.txt', sep='\\s+', skiprows=n,\n",
    "                       names=('date','time', 'SST', 'SSS', 'pCO2_sw', 'pCO2_air', 'xCO2_air', 'pH_sw', 'DOXY', 'CHL', 'NTU'))\n",
    "\n",
    "print(keo_alt.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Let's look at the beginning of the dataframe\")\n",
    "print(keo_alt.head(), '\\n')\n",
    "\n",
    "print(\"And the end of the dataframe.\")\n",
    "print(keo_alt.tail(10), '\\n')  # we can specify the number of rows we want to see in the parentheses.\n",
    "\n",
    "print(\"Let's take a look at some general statistics.\")\n",
    "print(keo_alt.describe())\n",
    "print(\"df.describe() will only show statistics on numerical data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define data time as index and add depth and month columns\n",
    "# Note, there is an issue with the code\n",
    "\n",
    "def surfdf (_df, date, time, dpth):\n",
    "    _df['datetime64_ns'] = pd.to_datetime(_df[date] + ' ' + _df[time]) # combine date and time into one column\n",
    "    _df.index = _df.datetime64_ns # set datetime column as index\n",
    "    _df['Depth'] = dpth # add depth of surface measurements as column\n",
    "    _df['month'] = _df['datetime64_ns'].dt.month.values #make month column\n",
    "    \n",
    "    return _df\n",
    "\n",
    "# apply function to dataframe\n",
    "\n",
    "df_keo = surfdf(keo_alt, 'date', 'time', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take look at the beginning of the new dataframe\n",
    "\n",
    "df_keo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the different data types of variables in the data frame\n",
    "print(df_keo.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping data\n",
    "We can remove rows or columns using the df.drop() function. You must specify which rows/columns will be dropped (either by index number or column name). axis=0 tells the function you will be dropping rows, axis=1 tells the function you will be dropping columns (you must specify an axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keo = df_keo.drop(['date', 'time', 'CHL', 'NTU', 'DOXY'], axis=1)\n",
    "df_keo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing NaN\n",
    "If you want to drop any row/column with missing data, you can use the df.dropna() function. You must specify an axis. axis=0 will remove any rows containing a NaN, axis=1 will drop any columns containing a NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Shape of dataframe before removing NaNs: \", df_keo.shape)\n",
    "df_keo_small = df_keo.dropna(axis=0)\n",
    "print(\"Shape of dataframe after removing rows containing NaNs\", df_keo_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and aggregating, transforming, and filtering those groups\n",
    "Data can be grouped together by values of a particular column using the df.groupby('column name'). The groupby function creates a special pandas object that does not work the same way as a dataframe (it is somewhat similar to a dictionary containing dataframes). So dataframe functions may not work on a group object, or may work differently than expected. Conversely, some of the functions described in this section only work on group objects, but not on dataframe objects.\n",
    "\n",
    "A group object is also iterable, so you can go through each group using a for loop. Ex.\n",
    "> for group in group_object: ...\n",
    "\n",
    "https://www.geeksforgeeks.org/pandas-groupby/\n",
    "\n",
    "A more thorough reference: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can group data using df.groupby(column). You can access individual groups using grp.get_group('group name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keo_monthgroup = df_keo.groupby('month')\n",
    "df_keo_monthgroup.get_group(1)  # note that this is accessing the group objected created by groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "We can use statistics on the groups by using grp.agg(statistics). You can use built in statistics or external functions that return a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average monthly values for dataset\n",
    "df_keo_avgmonth = df_keo.groupby('month').agg(['mean', np.std, 'count'])  # note that we can use more than built in functions\n",
    "df_keo_avgmonth # displays the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose to perform these statistics on only a subsection of the columns. (Note, here we also change how we perform the groupby and get the correct results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keo_avgmonth_2 = df_keo.groupby(by=df_keo['datetime64_ns'].dt.month).agg({'pCO2_sw': ['mean', 'std'], 'pCO2_air': ['mean', 'std', 'count']})\n",
    "df_keo_avgmonth_2 # displays the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is now nested (there is a primary key, like SST and SSS, and a secondary key, like mean and std). To access the secondary key, you must first access the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max monthly mean. Either version works\n",
    "print(df_keo_avgmonth.pCO2_sw['mean'].max())\n",
    "\n",
    "print(df_keo_avgmonth['pCO2_sw']['mean'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "We can also apply functions using grp.transform(func) to the groups. For example, we can standardize the columns by month rather than the entire dataset. The function must be able to handle all columns of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "standard_scalar = lambda x: (x - np.mean(x)) / np.std(x)\n",
    "df_keo_monthgroup.transform(standard_scalar)\n",
    "# type(df_keo_monthgroup.transform(standard_scalar).iloc[5, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "We can remove groups based on a filter. For example, if we want our group to have at least 750 pH mesurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keo_monthgroup.filter(lambda x: len(x['pH_sw']) - x['pH_sw'].isna().sum() > 750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at some general statistics to see what the filter has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keo_filter = df_keo_monthgroup.filter(lambda x: len(x['pH_sw']) - x['pH_sw'].isna().sum() > 750)\n",
    "df_keo_filter['month'] = df_keo_filter['datetime64_ns'].dt.month\n",
    "print(df_keo_filter.nunique(), '\\n')\n",
    "print(df_keo_filter.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "There are three different, major magic functions you can use in Jupyter Notebook which will determine how your figures appear.\n",
    "\n",
    "%matplotlib inline  <-- Keeps the figures within the jupyter notebook, but lacks interactability\n",
    "\n",
    "%matplotlib qt  <-- Opens figures up in a new window. Has interactability\n",
    "\n",
    "%matplotlib notebook  <-- Keeps the figures withing jupyter notebook and has interactability. Jupyter will complain if too many are open. Also, pandas quick plotting doesn't seem to work very well with notebook. \n",
    "\n",
    "inline is standardard, since it is static and doesn't take up much memory. I like notebook for presentations and to visualize more complicated data (for myself while investigating). I've found some functions only work with qt. Jupyter doesn't handle switching between these magic functions very well, so choose one per notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick plotting using Pandas\n",
    "We can use pandas' built in plotting (which is built on matplotlib) to make quick plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick plot\n",
    "%matplotlib inline\n",
    "df_keo.SST.plot()\n",
    "plt.title('SST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the datetime is used for the x-axis automatically since it was specified as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_keo.pCO2_sw.hist(bins=10, legend=True)\n",
    "df_keo.pCO2_air.hist(bins=20, legend=True)\n",
    "plt.legend(['pCO2 sw', 'pCO2 air'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_keo.boxplot(['pCO2_sw', 'pCO2_air', 'pH_sw'])\n",
    "plt.show()\n",
    "\n",
    "df_keo.boxplot('pH_sw', grid=False, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot monthly averages\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "im = ax.errorbar(df_keo_avgmonth.index, df_keo_avgmonth.SST['mean'], \n",
    "                df_keo_avgmonth.SST['std'], marker= 's', color='C0',elinewidth=1, linewidth=3)\n",
    "\n",
    "ax.set_xlabel('Month')#, fontsize=fs)\n",
    "ax.set_ylabel('SST')#, fontsize=fs)\n",
    "ax.set_xticks(np.arange(1, 13, step=2))\n",
    "\n",
    "plt.savefig('keo_avgmonth_sst.png', bbox_inches='tight', dpi=600)\n",
    "\n",
    "plt.show() # displays plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  # change how Jupyter notebook displays plots.\n",
    "\n",
    "# %matplotlib inline will show non-interactable plots (see above)\n",
    "# %matplotlib notebook will show interactable plots (see below)\n",
    "# %matplotlib qt will create a new window with the plot(s)\n",
    "\n",
    "# you cannont change between %matplotlib notebook and %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_error = np.random.randn(keo.shape[0]) / 5.\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 7))\n",
    "ax1.plot(keo['dtime'], keo['SST'], c='r')\n",
    "ax1.scatter(keo['dtime'], keo['SSS'], c='g')\n",
    "ax1.set_title('SST and SSS')\n",
    "ax1.legend(['SST', 'SSS'])\n",
    "\n",
    "ax2.errorbar(keo['dtime'], keo['SSS'], random_error, linestyle='None', ecolor='b', marker='*', markersize=2, c='r')\n",
    "ax2.set_title('SSS with error')\n",
    "ax2.set_xlabel('Timestamp', fontsize=20)\n",
    "ax2.set_ylim([32, 36])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a slightly different way to create subplots\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "im = ax.plot(df_keo.index, df_keo.SST)\n",
    "im2 = ax.plot(df_keo.index,df_keo.SSS)\n",
    "\n",
    "ax.set_ylabel('SST and SSS') # fontsize=fs)\n",
    "\n",
    "plt.savefig('keo_ts.png', bbox_inches='tight', dpi=600) # saves plot as image file\n",
    "plt.show() # displays plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(keo['dtime'], keo['pCO2 sw'], c=keo['SST'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('pCO2 sw')\n",
    "plt.colorbar(label='SST')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the figure\n",
    "\n",
    "fig2.savefig('KEO pCO2 sw with temperature.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining dataframes\n",
    "There are two different functions you will normally use: df.join(other_df) and pd.merge(df_1, df_2, on='column name').\n",
    "\n",
    "df.join(...) only joins on the index. The other dataframe's columns are appended to the end.\n",
    "\n",
    "pd.merge(df_1, df_2, on='column name') will join on a particular column (the column must be in both dataframes).\n",
    "\n",
    "A third and fourth method (concat and append) are provided below if you wish to add rows to your dataframe.\n",
    "\n",
    "https://realpython.com/pandas-merge-join-and-concat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pCO2 = keo[['dtime', 'pCO2 sw', 'pCO2 air']]\n",
    "print(df_pCO2.head(), '\\n')\n",
    "\n",
    "df_sst_sss = keo[['SSS', 'SST']]\n",
    "print(df_sst_sss.head(), '\\n')\n",
    "\n",
    "df_ph = keo[['dtime', 'pH']]\n",
    "print(df_ph.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ph = df_ph.join(df_sst_sss)\n",
    "print(df_ph.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_pCO2.set_index('dtime').join(df_ph.set_index('dtime'))\n",
    "print(df_joined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ph_short = df_ph.loc[2:100, :]\n",
    "print(\"We create a shortened dataframe to merge with df_pCO2 to show the effects of a merge.\")\n",
    "print(df_ph_short.head())\n",
    "print(df_ph_short.shape, '\\n')\n",
    "\n",
    "# note the how='left' here is only necessary because the dataframes do not have the same number of rows.\n",
    "df_merge = pd.merge(df_pCO2, df_ph_short, how='left', on='dtime')\n",
    "print(\"If we look at the first 5 rows of the merged dataframe, we see that pH, SSS, and SST have been filled with NaN where data is missing.\")\n",
    "print(df_merge.head(), '\\n')\n",
    "print(\"This is also true if we look at the tail of the dataframe.\")\n",
    "print(df_merge.tail(), '\\n')\n",
    "print(df_merge.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a short explination about join types\n",
    "Note above the two dataframes have different number of rows. If we do not specify the how variable, we will get an inner join. This means that only rows in which key values match in both dataframes will be used.\n",
    "\n",
    "If we specify how='left' (as above), we guarantee that all of the rows in the left dataframe (in this case df_pCO2) will be included, but not the right dataframe rows. So if a dtime exists in df_ph_short but not in df_pCO2, that dtime will not be included in the merge. Since the df_pCO2 is not missing any datetime values, this is acceptable. Also note, since df_ph_short has fewer rows than df_pCO2, missing values will be filled with NaN.\n",
    "\n",
    "If we specify how='right', we guarantee that all rows in the right dataframe (df_ph_short), but not the left dataframe rows. In this case, we would end up with a dataframe that has the same number of rows as df_ph_short.\n",
    "\n",
    "If we specify how='outer', we will include all rows from both dataframes. Any missing data from either dataframe will be filled with NaN.\n",
    "\n",
    "### additional note about on='dtime'\n",
    "If you have more than one key you would like to merge on (say you have several of the same dtimes but multiple sites), you can specify on=['dtime', 'site']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat\n",
    "pd.concat(...) has many uses. One that might be more useful to you is adding rows to an existing dataframe. I recommend you review the link provided at the beginning of this section if you would like to see the other uses.\n",
    "\n",
    "## Append\n",
    "df_1.append(df_2) can also be used to add rows to an existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ph_short_2 = df_ph.loc[250:795, :]\n",
    "\n",
    "print(\"\"\"We can use .concat() to add the rows of one dataframe to another.\n",
    "This is not the only function of concat, but concat is a bit more tricky to use than join or merge.\"\"\")\n",
    "df_ph_concat = pd.concat([df_ph_short, df_ph_short_2], axis=0)\n",
    "# note that axis=0 isn't necessary above (the default is axis=0). This is to illustrate that 0 will concat rows while 1 will concat columns.\n",
    "print(df_ph_concat, '\\n')\n",
    "\n",
    "print(\"Note that the index has not reset. To reset the index, we can use ignore_index=True\")\n",
    "print(pd.concat([df_ph_short, df_ph_short_2], axis=0, ignore_index=True), '\\n')\n",
    "\n",
    "\n",
    "print(\"We can also concatenate two dataframes along the columns by using df1.append(df_2)\")\n",
    "df_ph_append = df_ph_short.append(df_ph_short_2)\n",
    "print(df_ph_append)\n",
    "print(\"Note that the index did not reset. We can use .reset_index() to fix this issue (if it is an issue).\")\n",
    "df_ph_append = df_ph_append.reset_index()\n",
    "print(df_ph_append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
